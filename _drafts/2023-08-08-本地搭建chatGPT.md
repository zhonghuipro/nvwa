---

layout: post

title: "本地搭建chatGPT"

date: 2023-08-08 14:55:21 +0800

categories:

tags:
   
---


参考文章：

原文
https://towardsdatascience.com/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8

翻译
https://zhuanlan.zhihu.com/p/644701608

github

https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference

模型下载

https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML



# 运行环境

python3 -m venv tenv

source tenv/bin/activate

pip install llama2-wrapper  --index-url https://mirrors.aliyun.com/pypi/simple

git clone git@github.com:liltom-eth/llama2-webui.git

cd llama2-webui 


vi .env
```
MODEL_PATH = "/Users/hui/Downloads/Chinese-Llama-2-7b.ggmlv3.q8_0.bin"
LOAD_IN_8BIT = True
LOAD_IN_4BIT = False
LLAMA_CPP = True

MAX_MAX_NEW_TOKENS = 2048
DEFAULT_MAX_NEW_TOKENS = 1024
MAX_INPUT_TOKEN_LENGTH = 4000

DEFAULT_SYSTEM_PROMPT = "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."
```


https://zhuanlan.zhihu.com/p/645426799


python3 convert.py  /app/data/FlagAlpha-Llama2-Chinese-7b-Chat/ --outfile /app/data/FlagAlpha-Llama2-Chinese-7b-Chat-ggml.bin
